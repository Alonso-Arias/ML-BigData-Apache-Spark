{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "Welcome to exercise one of week four of \u201cApache Spark for Scalable Machine Learning on BigData\u201d. In this exercise we\u2019ll work on classification.\n\nLet\u2019s create our DataFrame again:"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# delete files from previous runs\n!rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n    \n# create a dataframe out of it\ndf = spark.read.parquet('hmp.parquet')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')",
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200219191651-0000\nKERNEL_ID = e0951f42-b86e-4aa8-b002-952f851f6c78\n--2020-02-19 19:16:54--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 140.82.113.3\nConnecting to github.com (github.com)|140.82.113.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/coursera/master/hmp.parquet [following]\n--2020-02-19 19:16:54--  https://raw.githubusercontent.com/IBM/coursera/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.8.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.8.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: 'hmp.parquet'\n\n100%[======================================>] 932,997     --.-K/s   in 0.03s   \n\n2020-02-19 19:16:55 (27.6 MB/s) - 'hmp.parquet' saved [932997/932997]\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Dado que este es un aprendizaje supervisado, dividamos nuestros datos en conjunto de entrenamiento (80%) y prueba (20%)"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "splits = df.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]",
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Nuevamente, podemos reutilizar nuestro Pipline de ingenier\u00eda de caracter\u00edsticas",
            "attachments": {}
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.feature import StringIndexer, OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\n\nindexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n                                  outputCol=\"features\")\n\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)",
            "execution_count": 3,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Ahora usamos LogisticRegression, un clasificador lineal simple y b\u00e1sico para obtener una l\u00ednea base de rendimiento de clasificaci\u00f3n."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\npipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer,lr])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)",
            "execution_count": 4,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Si observamos el esquema del marco de datos de predicci\u00f3n, vemos que hay una columna adicional llamada predicci\u00f3n que contiene la mejor conjetura para las predicciones de clase o modelo."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "prediction.printSchema()",
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "root\n |-- x: integer (nullable = true)\n |-- y: integer (nullable = true)\n |-- z: integer (nullable = true)\n |-- source: string (nullable = true)\n |-- class: string (nullable = true)\n |-- label: double (nullable = false)\n |-- features: vector (nullable = true)\n |-- features_norm: vector (nullable = true)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = false)\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Vamos a evaluar el rendimiento mediante el uso de una funcionalidad integrada de Apache SparkML"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nMulticlassClassificationEvaluator().setMetricName(\"accuracy\").evaluate(prediction)",
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 6,
                    "data": {
                        "text/plain": "0.2042519260573662"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Entonces obtenemos el 20% correcto. Esto no es malo para una l\u00ednea de base. Tenga en cuenta que adivinar al azar nos dar\u00eda solo el 7%. Por supuesto que necesitamos mejorar. Es posible que tenga avisos de que estamos tratando con una serie de tiempo aqu\u00ed. Y no estamos haciendo uso de ese hecho en este momento, ya que miramos cada ejemplo de entrenamiento solo individualmente. Pero esto est\u00e1 bien por ahora. Cursos m\u00e1s avanzados como \"Aprendizaje autom\u00e1tico avanzado y procesamiento de se\u00f1ales\" (https://www.coursera.org/learn/advanced-machine-learning-signal-processing/) le ense\u00f1ar\u00e1n c\u00f3mo mejorar la precisi\u00f3n al 100% utilizando algoritmos como la transformaci\u00f3n de Fourier o la transformaci\u00f3n wavelet. Pero omita esto por ahora. En la siguiente celda, utilice el clasificador RandomForest (es posible que deba jugar con el par\u00e1metro \"numTrees\") en la celda de c\u00f3digo a continuaci\u00f3n. Debe obtener una precisi\u00f3n de alrededor del 44%. Puede encontrar m\u00e1s informaci\u00f3n sobre RandomForest aqu\u00ed:\n\nhttps://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier"
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python36",
            "display_name": "Python 3.6 with Spark",
            "language": "python3"
        },
        "language_info": {
            "mimetype": "text/x-python",
            "nbconvert_exporter": "python",
            "name": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8",
            "file_extension": ".py",
            "codemirror_mode": {
                "version": 3,
                "name": "ipython"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}