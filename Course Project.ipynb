{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Bienvenido al proyecto final de \"Apache Spark para el aprendizaje autom\u00e1tico escalable en BigData\". En esta tarea, analizar\u00e1 un conjunto de datos del mundo real y aplicar\u00e1 el aprendizaje autom\u00e1tico con Apache Spark.\n\nPara aprobar, debe implementar un c\u00f3digo (como se describe en la secci\u00f3n de instrucciones en Coursera) y finalmente responder un cuestionario en la plataforma de Coursera.\n\nComencemos descargando el conjunto de datos y creando un marco de datos. Este conjunto de datos se puede encontrar en DAX, IBM Data Asset Exchange y se puede descargar de forma gratuita.\nhttps://developer.ibm.com/exchanges/data/all/jfk-weather-data/\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# eliminar archivos de ejecuciones anteriores\n!rm -f jfk_weather*\n\n# descargar el archivo que contiene los datos en formato CSV\n!wget http://max-training-data.s3-api.us-geo.objectstorage.softlayer.net/noaa-weather/jfk_weather.tar.gz\n\n# extraer los datos\n!tar xvfz jfk_weather.tar.gz\n    \n# crear un marco de datos a partir de \u00e9l utilizando la primera fila como nombres de campo e intentando inferir un esquema basado en el contenido\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv('jfk_weather.csv')\n\n# registrar una tabla de consulta correspondiente\ndf.createOrReplaceTempView('df')",
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200219192531-0000\nKERNEL_ID = 1193ebb4-1dbf-4ffe-b734-6337dbd631cf\n--2020-02-19 19:25:34--  http://max-training-data.s3-api.us-geo.objectstorage.softlayer.net/noaa-weather/jfk_weather.tar.gz\nResolving max-training-data.s3-api.us-geo.objectstorage.softlayer.net (max-training-data.s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to max-training-data.s3-api.us-geo.objectstorage.softlayer.net (max-training-data.s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2575759 (2.5M) [application/x-tar]\nSaving to: 'jfk_weather.tar.gz'\n\n100%[======================================>] 2,575,759   --.-K/s   in 0.03s   \n\n2020-02-19 19:25:34 (71.3 MB/s) - 'jfk_weather.tar.gz' saved [2575759/2575759]\n\n./._jfk_weather.csv\njfk_weather.csv\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "El conjunto de datos contiene algunos valores nulos, por lo tanto, la inferencia de esquema no funcion\u00f3 correctamente para todas las columnas, adem\u00e1s, una columna conten\u00eda caracteres finales, por lo que primero debemos limpiar el conjunto de datos. Esta es una tarea normal en cualquier proyecto de ciencia de datos ya que sus datos nunca est\u00e1n limpios, no se preocupe si no comprende todo el c\u00f3digo, no se le preguntar\u00e1 al respecto."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import random\nrandom.seed(42)\n\nfrom pyspark.sql.functions import translate, col\n\ndf_cleaned = df \\\n    .withColumn(\"HOURLYWindSpeed\", df.HOURLYWindSpeed.cast('double')) \\\n    .withColumn(\"HOURLYWindDirection\", df.HOURLYWindDirection.cast('double')) \\\n    .withColumn(\"HOURLYStationPressure\", translate(col(\"HOURLYStationPressure\"), \"s,\", \"\")) \\\n    .withColumn(\"HOURLYPrecip\", translate(col(\"HOURLYPrecip\"), \"s,\", \"\")) \\\n    .withColumn(\"HOURLYRelativeHumidity\", translate(col(\"HOURLYRelativeHumidity\"), \"*\", \"\")) \\\n    .withColumn(\"HOURLYDRYBULBTEMPC\", translate(col(\"HOURLYDRYBULBTEMPC\"), \"*\", \"\")) \\\n\ndf_cleaned =   df_cleaned \\\n                    .withColumn(\"HOURLYStationPressure\", df_cleaned.HOURLYStationPressure.cast('double')) \\\n                    .withColumn(\"HOURLYPrecip\", df_cleaned.HOURLYPrecip.cast('double')) \\\n                    .withColumn(\"HOURLYRelativeHumidity\", df_cleaned.HOURLYRelativeHumidity.cast('double')) \\\n                    .withColumn(\"HOURLYDRYBULBTEMPC\", df_cleaned.HOURLYDRYBULBTEMPC.cast('double')) \\\n\ndf_filtered = df_cleaned.filter(\"\"\"\n    HOURLYWindSpeed <> 0\n    and HOURLYWindDirection <> 0\n    and HOURLYStationPressure <> 0\n    and HOURLYPressureTendency <> 0\n    and HOURLYPressureTendency <> 0\n    and HOURLYPrecip <> 0\n    and HOURLYRelativeHumidity <> 0\n    and HOURLYDRYBULBTEMPC <> 0\n\"\"\")",
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Queremos predecir el valor de una columna en funci\u00f3n de otras. A veces es \u00fatil imprimir una matriz de correlaci\u00f3n."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.feature import VectorAssembler\nvectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYWindDirection\",\"HOURLYStationPressure\"],\n                                  outputCol=\"features\")\ndf_pipeline = vectorAssembler.transform(df_filtered)\nfrom pyspark.ml.stat import Correlation\nCorrelation.corr(df_pipeline,\"features\").head()[0].toArray()",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 3,
                    "data": {
                        "text/plain": "array([[ 1.        ,  0.06306013, -0.4204518 ],\n       [ 0.06306013,  1.        , -0.19199348],\n       [-0.4204518 , -0.19199348,  1.        ]])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "As we can see, HOURLYWindSpeed and HOURLYWindDirection correlate with 0.06306013 whereas HOURLYWindSpeed  and HOURLYStationPressure correlate with -0.4204518, this is a good sign if we want to predict HOURLYWindSpeed from HOURLYWindDirection and HOURLYStationPressure.\nSince this is supervised learning, let\u2019s split our data into train (80%) and test (20%) set."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "splits = df_filtered.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]",
            "execution_count": 4,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Again, we can re-use our feature engineering pipeline"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.feature import StringIndexer, OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\nfrom pyspark.ml import Pipeline\n\nvectorAssembler = VectorAssembler(inputCols=[\n                                    \"HOURLYWindDirection\",\n                                    \"ELEVATION\",\n                                    \"HOURLYStationPressure\"],\n                                  outputCol=\"features\")\n\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)",
            "execution_count": 5,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Ahora definimos una funci\u00f3n para evaluar nuestro rendimiento de predicci\u00f3n de regresi\u00f3n. Estamos usando RMSE (Root Mean Squared Error) aqu\u00ed, cuanto m\u00e1s peque\u00f1o, mejor ...\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def regression_metrics(prediction):\n    from pyspark.ml.evaluation import RegressionEvaluator\n    evaluator = RegressionEvaluator(\n    labelCol=\"HOURLYWindSpeed\", predictionCol=\"prediction\", metricName=\"rmse\")\n    rmse = evaluator.evaluate(prediction)\n    print(\"RMSE on test data = %g\" % rmse)",
            "execution_count": 6,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Primero, ejecutemos un modelo de regresi\u00f3n lineal para construir una l\u00ednea base.\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#LR1\n\nfrom pyspark.ml.regression import LinearRegression\n\n\nlr = LinearRegression(labelCol=\"HOURLYWindSpeed\", featuresCol='features', maxIter=100, regParam=0.0, elasticNetParam=0.0)\npipeline = Pipeline(stages=[vectorAssembler, normalizer,lr])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nregression_metrics(prediction)",
            "execution_count": 7,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "RMSE on test data = 6.33073\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Ahora intentaremos un Gradient Boosted Tree Regressor"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#GBT1\n\nfrom pyspark.ml.regression import GBTRegressor\ngbt = GBTRegressor(labelCol=\"HOURLYWindSpeed\", maxIter=100)\npipeline = Pipeline(stages=[vectorAssembler, normalizer,gbt])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nregression_metrics(prediction)",
            "execution_count": 8,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "RMSE on test data = 7.01428\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Now let\u2019s switch gears. Previously, we tried to predict HOURLYWindSpeed, but now we predict HOURLYWindDirection. In order to turn this into a classification problem we discretize the value using the Bucketizer. The new feature is called HOURLYWindDirectionBucketized."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.feature import Bucketizer, OneHotEncoder\nbucketizer = Bucketizer(splits=[ 0, 180, float('Inf') ],inputCol=\"HOURLYWindDirection\", outputCol=\"HOURLYWindDirectionBucketized\")\nencoder = OneHotEncoder(inputCol=\"HOURLYWindDirectionBucketized\", outputCol=\"HOURLYWindDirectionOHE\")\n",
            "execution_count": 9,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Nuevamente, definimos una funci\u00f3n para evaluar c\u00f3mo nos desempe\u00f1amos. Aqu\u00ed solo usamos la medida de precisi\u00f3n que nos da la fracci\u00f3n de ejemplos correctamente clasificados. De nuevo, 0 es malo, 1 es bueno."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def classification_metrics(prediction):\n    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n    mcEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"HOURLYWindDirectionBucketized\")\n    accuracy = mcEval.evaluate(prediction)\n    print(\"Accuracy on test data = %g\" % accuracy)",
            "execution_count": 10,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Nuevamente, para la l\u00ednea de base usamos LogisticRegression."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#LGReg1\n\nfrom pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=10)\n#,\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"\n\nvectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\"],\n                                  outputCol=\"features\")\n\npipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,lr])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nclassification_metrics(prediction)",
            "execution_count": 11,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Accuracy on test data = 0.613208\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Probemos otros algoritmos y veamos si aumenta el rendimiento del modelo. Tambi\u00e9n es importante ajustar otros par\u00e1metros como par\u00e1metros de algoritmos individuales (por ejemplo, n\u00famero de \u00e1rboles para RandomForest) o par\u00e1metros Pipline de ingenier\u00eda de caracter\u00edsticas, por ejemplo relaci\u00f3n de divisi\u00f3n de tren / prueba, normalizaci\u00f3n, desbarbado, ..."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#RF1\n\nfrom pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(labelCol=\"HOURLYWindDirectionBucketized\", numTrees=30)\n\nvectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n                                  outputCol=\"features\")\n\npipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,rf])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nclassification_metrics(prediction)",
            "execution_count": 12,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Accuracy on test data = 0.698113\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#GBT2\n\nfrom pyspark.ml.classification import GBTClassifier\ngbt = GBTClassifier(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=100)\n\nvectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n                                  outputCol=\"features\")\n\npipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,gbt])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nclassification_metrics(prediction)",
            "execution_count": 13,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Accuracy on test data = 0.688679\n",
                    "name": "stdout"
                }
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python36",
            "display_name": "Python 3.6 with Spark",
            "language": "python3"
        },
        "language_info": {
            "mimetype": "text/x-python",
            "nbconvert_exporter": "python",
            "name": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8",
            "file_extension": ".py",
            "codemirror_mode": {
                "version": 3,
                "name": "ipython"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}